{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "### Yanan Zhou | 4366462176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "A general struction of NLP:\n",
    "https://realpython.com/sentiment-analysis-python/#using-natural-language-processing-to-preprocess-and-clean-text-data\n",
    "\n",
    "How to read text files: https://www.pythontutorial.net/python-basics/python-read-text-file/\n",
    "\n",
    "How to remove puncutation and numbers in a string: https://datagy.io/python-remove-punctuation-from-string/#:~:text=Conclusion-,Use%20Python%20to%20Remove%20Punctuation%20from%20a%20String%20with%20Translate,translate()%20method.\n",
    "\n",
    "Padding and truncating: https://towardsdatascience.com/nlp-preparing-text-for-deep-learning-model-using-tensorflow2-461428138657#:~:text=Padding%20at%20the%20beginning%20allows,careful%20consideration%20and%20business%20knowledge\n",
    "\n",
    "Word embedding: (1)https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "(2)https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "Building MLP: https://machinelearningmastery.com/build-multi-layer-perceptron-neural-network-models-keras/\n",
    "\n",
    "\n",
    "Building CNN: (1)https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D (2)https://keras.io/api/layers/pooling_layers/max_pooling1d/\n",
    "\n",
    "Building LSTM: https://keras.io/api/layers/recurrent_layers/lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) In this problem, we are trying to build a classifier to analyze the sentiment of reviews. You are provided with text data in two folders: one folder involves positive reviews, and one folder involves negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_file = os.listdir('../data/neg')\n",
    "pos_file = os.listdir('../data/pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_review(path, file_list, category):\n",
    "    \n",
    "    '''\n",
    "    path: the work path the files stored in\n",
    "    file_list: a list containing the names of all the the files we would like to read\n",
    "    category: the classification class of these list of files\n",
    "    \n",
    "    '''\n",
    "    # read the reviews in each file\n",
    "    ls = []\n",
    "    for i in range(len(file_list)):\n",
    "        with open(path + file_list[i], 'r') as f:\n",
    "            d = f.read()\n",
    "            f.close()\n",
    "          \n",
    "        d = d.replace(\"\\n\", \"\")\n",
    "        d = d.replace(\"\\\"\", \"\")\n",
    "    \n",
    "        # save each review and sentiment \n",
    "        ls.append([file_list[i][:-4], d, category])\n",
    "    \n",
    "    df = pd.DataFrame(ls, columns = ['id', 'review', 'sentiment'])\n",
    "        \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv676_22202</td>\n",
       "      <td>bad . bad . bad . that one word seems to prett...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv839_22807</td>\n",
       "      <td>isn't it the ultimate sign of a movie's cinema...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv155_7845</td>\n",
       "      <td>gordy  is not a movie , it is a 90-minute-lo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv465_23401</td>\n",
       "      <td>disconnect the phone line . don't accept the c...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv398_17047</td>\n",
       "      <td>when robert forster found himself famous again...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>cv588_13008</td>\n",
       "      <td>one of the funniest carry on movies and the th...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>cv734_21568</td>\n",
       "      <td>i remember making a pact , right after `patch ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>cv491_12145</td>\n",
       "      <td>barely scrapping by playing at a nyc piano bar...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>cv647_13691</td>\n",
       "      <td>if the current trends of hollywood filmmaking ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>cv665_29538</td>\n",
       "      <td>capsule : the director of cure brings a weird ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                             review sentiment\n",
       "0     cv676_22202  bad . bad . bad . that one word seems to prett...       neg\n",
       "1     cv839_22807  isn't it the ultimate sign of a movie's cinema...       neg\n",
       "2      cv155_7845    gordy  is not a movie , it is a 90-minute-lo...       neg\n",
       "3     cv465_23401  disconnect the phone line . don't accept the c...       neg\n",
       "4     cv398_17047  when robert forster found himself famous again...       neg\n",
       "...           ...                                                ...       ...\n",
       "1995  cv588_13008  one of the funniest carry on movies and the th...       pos\n",
       "1996  cv734_21568  i remember making a pact , right after `patch ...       pos\n",
       "1997  cv491_12145  barely scrapping by playing at a nyc piano bar...       pos\n",
       "1998  cv647_13691  if the current trends of hollywood filmmaking ...       pos\n",
       "1999  cv665_29538  capsule : the director of cure brings a weird ...       pos\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the negtive and postive reviews into dataframe\n",
    "df_neg_org = read_review('../data/neg/', neg_file, 'neg')\n",
    "df_pos_org = read_review('../data/pos/', pos_file, 'pos')\n",
    "\n",
    "org_review = df_neg_org.append(df_pos_org)\n",
    "org_review.index = range(len(df_neg_org) + len(df_pos_org))\n",
    "\n",
    "org_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Data Exploration and Pre-processing\n",
    "### i. You can use binary encoding for the sentiments , i.e y = 1 for positive sentiments and y = −1 for negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv676_22202</td>\n",
       "      <td>bad . bad . bad . that one word seems to prett...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv839_22807</td>\n",
       "      <td>isn't it the ultimate sign of a movie's cinema...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv155_7845</td>\n",
       "      <td>gordy  is not a movie , it is a 90-minute-lo...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv465_23401</td>\n",
       "      <td>disconnect the phone line . don't accept the c...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv398_17047</td>\n",
       "      <td>when robert forster found himself famous again...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>cv588_13008</td>\n",
       "      <td>one of the funniest carry on movies and the th...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>cv734_21568</td>\n",
       "      <td>i remember making a pact , right after `patch ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>cv491_12145</td>\n",
       "      <td>barely scrapping by playing at a nyc piano bar...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>cv647_13691</td>\n",
       "      <td>if the current trends of hollywood filmmaking ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>cv665_29538</td>\n",
       "      <td>capsule : the director of cure brings a weird ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                             review  \\\n",
       "0     cv676_22202  bad . bad . bad . that one word seems to prett...   \n",
       "1     cv839_22807  isn't it the ultimate sign of a movie's cinema...   \n",
       "2      cv155_7845    gordy  is not a movie , it is a 90-minute-lo...   \n",
       "3     cv465_23401  disconnect the phone line . don't accept the c...   \n",
       "4     cv398_17047  when robert forster found himself famous again...   \n",
       "...           ...                                                ...   \n",
       "1995  cv588_13008  one of the funniest carry on movies and the th...   \n",
       "1996  cv734_21568  i remember making a pact , right after `patch ...   \n",
       "1997  cv491_12145  barely scrapping by playing at a nyc piano bar...   \n",
       "1998  cv647_13691  if the current trends of hollywood filmmaking ...   \n",
       "1999  cv665_29538  capsule : the director of cure brings a weird ...   \n",
       "\n",
       "     sentiment  Sentiment  \n",
       "0          neg         -1  \n",
       "1          neg         -1  \n",
       "2          neg         -1  \n",
       "3          neg         -1  \n",
       "4          neg         -1  \n",
       "...        ...        ...  \n",
       "1995       pos          1  \n",
       "1996       pos          1  \n",
       "1997       pos          1  \n",
       "1998       pos          1  \n",
       "1999       pos          1  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_review['Sentiment'] = org_review['sentiment'].replace(('neg', 'pos'), (-1, 1))\n",
    "org_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. The data are pretty clean. Remove the punctuation and numbers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove punctuation and numbers\n",
    "def remove_punc_num(text_list):\n",
    "    \n",
    "    '''\n",
    "    text_list: a list of text data\n",
    "    \n",
    "    '''\n",
    "    ls = []\n",
    "    for i in range(len(text_list)):\n",
    "        new = text_list[i].translate(str.maketrans('', '', string.punctuation))\n",
    "        new = new.translate(str.maketrans('', '', string.digits))\n",
    "        ls.append(new)\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv676_22202</td>\n",
       "      <td>bad  bad  bad  that one word seems to pretty m...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv839_22807</td>\n",
       "      <td>isnt it the ultimate sign of a movies cinemati...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv155_7845</td>\n",
       "      <td>gordy  is not a movie  it is a minutelong  s...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv465_23401</td>\n",
       "      <td>disconnect the phone line  dont accept the cha...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv398_17047</td>\n",
       "      <td>when robert forster found himself famous again...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>cv588_13008</td>\n",
       "      <td>one of the funniest carry on movies and the th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>cv734_21568</td>\n",
       "      <td>i remember making a pact  right after patch ad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>cv491_12145</td>\n",
       "      <td>barely scrapping by playing at a nyc piano bar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>cv647_13691</td>\n",
       "      <td>if the current trends of hollywood filmmaking ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>cv665_29538</td>\n",
       "      <td>capsule  the director of cure brings a weird a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                             review  \\\n",
       "0     cv676_22202  bad  bad  bad  that one word seems to pretty m...   \n",
       "1     cv839_22807  isnt it the ultimate sign of a movies cinemati...   \n",
       "2      cv155_7845    gordy  is not a movie  it is a minutelong  s...   \n",
       "3     cv465_23401  disconnect the phone line  dont accept the cha...   \n",
       "4     cv398_17047  when robert forster found himself famous again...   \n",
       "...           ...                                                ...   \n",
       "1995  cv588_13008  one of the funniest carry on movies and the th...   \n",
       "1996  cv734_21568  i remember making a pact  right after patch ad...   \n",
       "1997  cv491_12145  barely scrapping by playing at a nyc piano bar...   \n",
       "1998  cv647_13691  if the current trends of hollywood filmmaking ...   \n",
       "1999  cv665_29538  capsule  the director of cure brings a weird a...   \n",
       "\n",
       "      Sentiment  \n",
       "0            -1  \n",
       "1            -1  \n",
       "2            -1  \n",
       "3            -1  \n",
       "4            -1  \n",
       "...         ...  \n",
       "1995          1  \n",
       "1996          1  \n",
       "1997          1  \n",
       "1998          1  \n",
       "1999          1  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove number and punctuation in the reviews\n",
    "new_word_list = remove_punc_num(org_review['review'].tolist())\n",
    "new_review = org_review.drop(columns = ['sentiment']).copy()\n",
    "new_review['review'] = new_word_list\n",
    "new_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. The name of each text file starts with cv number. Use text files 0-699 in each class for training and 700-999 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the test set index\n",
    "test_idx = []\n",
    "for i in range(700, 1000):\n",
    "    for st in new_review['id'].tolist():\n",
    "        if int(st[2:5]) == i:\n",
    "            test_idx.append(new_review['id'].tolist().index(st))\n",
    "len(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1400, 3), (600, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test\n",
    "Train = new_review.drop(test_idx)\n",
    "Test = new_review.iloc[test_idx, :]\n",
    "\n",
    "Train.shape, Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Count the number of unique words in the whole dataset (train + test) and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46830\n"
     ]
    }
   ],
   "source": [
    "# save the 2000 reviews to a list\n",
    "docs = new_review['review'].tolist()\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "# count the unique words in the whole dataset\n",
    "print(len(t.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Calculate the average review length and the standard deviation of review lengths. Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate length of a review\n",
    "def length_review(text):\n",
    "    word_list = text_to_word_sequence(text)\n",
    "    length = len(word_list)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate average and standard deviation of review lengths\n",
    "def avg_std_length(text_list):\n",
    "    ls = []\n",
    "    for txt in text_list:\n",
    "        length = length_review(txt)\n",
    "        ls.append(length)\n",
    "    \n",
    "    avg = np.mean(ls)\n",
    "    std = np.std(ls)\n",
    "    \n",
    "    return ls, avg, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average review length of the entire dataset is 644.3555\n",
      "The standard deviation of review lengths of entire dataset is 284.97987142910637\n"
     ]
    }
   ],
   "source": [
    "# report the avg and std of entire dataset\n",
    "ls_total, avg_total, std_total = avg_std_length(docs)\n",
    "print('The average review length of the entire dataset is ' + str(avg_total))\n",
    "print('The standard deviation of review lengths of entire dataset is ' + str(std_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average review length of training set is 641.4178571428571\n",
      "The standard deviation of review lengths of training set is 285.0965181848464\n"
     ]
    }
   ],
   "source": [
    "# report the avg and std of Train set\n",
    "ls_train, avg_train, std_train = avg_std_length(Train['review'].tolist())\n",
    "print('The average review length of training set is ' + str(avg_train))\n",
    "print('The standard deviation of review lengths of training set is ' + str(std_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average review length of testing set is 651.21\n",
      "The standard deviation of review lengths of testing set is 284.58960961356263\n"
     ]
    }
   ],
   "source": [
    "# report the avg and std of Test set\n",
    "ls_test, avg_test, std_test = avg_std_length(Test['review'].tolist())\n",
    "print('The average review length of testing set is ' + str(avg_test))\n",
    "print('The standard deviation of review lengths of testing set is ' + str(std_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to plot histogram\n",
    "def hist_length(name, length_list):\n",
    "    \n",
    "    '''\n",
    "    name: the title of this list (i.e. 'train', 'test', 'total')\n",
    "    length_list: a list containing the length of each review\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    plt.figure(figsize = (7, 5))\n",
    "    sns.histplot(length_list)\n",
    "    \n",
    "    plt.xlabel('Review Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim([0, 2500])\n",
    "    plt.ylim([0, 200])\n",
    "    \n",
    "    plt.title('The Histogram of Review Length --- ' + name)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the histogram of entire dataset\n",
    "hist_length('whole Dataset', ls_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the histogram of training set\n",
    "hist_length('Training Set', ls_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the histogram of training set\n",
    "hist_length('Testing Set', ls_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vii. To represent each text (= data point), there are many ways. In NLP/Deep Learning terminology, this task is called tokenization. It is common to represent text using popularity/rank of words in text. The most common word in the text will be represented as 1, the second most common word will be represented as 2, etc. Tokenize each text document using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(df):\n",
    "    \n",
    "    # get the rank of all the numbers appearing in a dataframe\n",
    "    \n",
    "    ls = []\n",
    "    for i in range(len(df)):\n",
    "        col = df.iloc[i, :].tolist()\n",
    "        ls.extend(col)\n",
    "        \n",
    "    ls_unique = list(set(ls))\n",
    "    ls_unique.sort(reverse = True)\n",
    "    \n",
    "    rk = {}\n",
    "    for i in range(len(ls_unique)):\n",
    "        rk[i + 1] = ls_unique[i] \n",
    "    \n",
    "    return rk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for encoding\n",
    "def rank_encoding(docs, tokenizer):\n",
    "    \n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    \n",
    "    # convert the text to matrix by counting word frequency\n",
    "    encoded_docs = tokenizer.texts_to_matrix(docs, mode = 'count')\n",
    "    df_encoded_docs = pd.DataFrame(encoded_docs).iloc[:, 1:]\n",
    "    df_encoded_docs.columns = list(tokenizer.word_index.keys())\n",
    "    \n",
    "    # convert word frequency to ranking\n",
    "    df_rank_docs = df_encoded_docs.copy()\n",
    "    rk = get_rank(df_rank_docs)\n",
    "    for key in rk.keys():\n",
    "        df_rank_docs = df_rank_docs.replace(rk[key], key)\n",
    "    \n",
    "    return rk, df_rank_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>a</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>to</th>\n",
       "      <th>is</th>\n",
       "      <th>in</th>\n",
       "      <th>that</th>\n",
       "      <th>it</th>\n",
       "      <th>as</th>\n",
       "      <th>...</th>\n",
       "      <th>chung</th>\n",
       "      <th>niftiest</th>\n",
       "      <th>nonsupernatural</th>\n",
       "      <th>mildewlike</th>\n",
       "      <th>taguchis</th>\n",
       "      <th>disturbances</th>\n",
       "      <th>lifeforce</th>\n",
       "      <th>hunches</th>\n",
       "      <th>junichiro</th>\n",
       "      <th>semilighting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>95.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>77.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>108.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>72.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>88.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 46830 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        the      a    and     of     to     is     in   that     it     as  \\\n",
       "0      61.0   89.0  105.0   97.0   87.0  105.0  103.0  102.0  104.0  110.0   \n",
       "1      93.0  103.0  112.0  105.0  107.0  112.0  114.0  117.0  112.0  119.0   \n",
       "2      96.0  104.0  100.0  109.0  103.0  105.0  118.0  115.0  112.0  119.0   \n",
       "3      80.0  104.0  105.0  104.0  105.0  107.0  113.0  115.0  120.0  113.0   \n",
       "4     100.0  112.0  111.0  111.0  111.0  114.0  117.0  117.0  110.0  118.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1995   95.0  111.0  100.0  110.0  111.0  111.0  114.0  119.0  117.0  112.0   \n",
       "1996   77.0  106.0  102.0   92.0  108.0  108.0  111.0  114.0  116.0  117.0   \n",
       "1997  108.0   98.0  111.0  113.0  116.0  105.0  115.0  117.0  118.0  121.0   \n",
       "1998   72.0  108.0  102.0   90.0  105.0  109.0  103.0  107.0  119.0  115.0   \n",
       "1999   88.0  103.0  100.0  104.0   93.0  105.0  113.0  115.0  116.0  120.0   \n",
       "\n",
       "      ...  chung  niftiest  nonsupernatural  mildewlike  taguchis  \\\n",
       "0     ...  121.0     121.0            121.0       121.0     121.0   \n",
       "1     ...  121.0     121.0            121.0       121.0     121.0   \n",
       "2     ...  121.0     121.0            121.0       121.0     121.0   \n",
       "3     ...  121.0     121.0            121.0       121.0     121.0   \n",
       "4     ...  121.0     121.0            121.0       121.0     121.0   \n",
       "...   ...    ...       ...              ...         ...       ...   \n",
       "1995  ...  121.0     121.0            121.0       121.0     121.0   \n",
       "1996  ...  121.0     121.0            121.0       121.0     121.0   \n",
       "1997  ...  121.0     121.0            121.0       121.0     121.0   \n",
       "1998  ...  120.0     121.0            121.0       121.0     121.0   \n",
       "1999  ...  121.0     120.0            120.0       120.0     120.0   \n",
       "\n",
       "      disturbances  lifeforce  hunches  junichiro  semilighting  \n",
       "0            121.0      121.0    121.0      121.0         121.0  \n",
       "1            121.0      121.0    121.0      121.0         121.0  \n",
       "2            121.0      121.0    121.0      121.0         121.0  \n",
       "3            121.0      121.0    121.0      121.0         121.0  \n",
       "4            121.0      121.0    121.0      121.0         121.0  \n",
       "...            ...        ...      ...        ...           ...  \n",
       "1995         121.0      121.0    121.0      121.0         121.0  \n",
       "1996         121.0      121.0    121.0      121.0         121.0  \n",
       "1997         121.0      121.0    121.0      121.0         121.0  \n",
       "1998         121.0      121.0    121.0      121.0         121.0  \n",
       "1999         120.0      120.0    120.0      120.0         120.0  \n",
       "\n",
       "[2000 rows x 46830 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# represent each text document using popularity/rank of words in text\n",
    "rk_dict, df_rank_docs = rank_encoding(docs, Tokenizer())\n",
    "\n",
    "df_rank_docs['id'] = new_review['id'].copy()\n",
    "df_rank_docs['sentiment'] = new_review['Sentiment'].copy()\n",
    "df_rank_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viii. Select a review length L that 70% of the reviews have a length below it. If you feel more adventurous, set the threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = int(np.quantile(ls_total, 0.70))\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ix. Truncate reviews longer than L words and zero-pad reviews shorter than L so that all texts (= data points) are of length L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>637</td>\n",
       "      <td>136</td>\n",
       "      <td>5</td>\n",
       "      <td>266</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>977</td>\n",
       "      <td>1633</td>\n",
       "      <td>5</td>\n",
       "      <td>129</td>\n",
       "      <td>264</td>\n",
       "      <td>11</td>\n",
       "      <td>cv676_22202</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1711</td>\n",
       "      <td>1550</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>1008</td>\n",
       "      <td>8629</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cv839_22807</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8124</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>17379</td>\n",
       "      <td>17380</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cv155_7845</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20940</td>\n",
       "      <td>1</td>\n",
       "      <td>1446</td>\n",
       "      <td>337</td>\n",
       "      <td>119</td>\n",
       "      <td>1760</td>\n",
       "      <td>1</td>\n",
       "      <td>4788</td>\n",
       "      <td>76</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cv465_23401</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>463</td>\n",
       "      <td>5133</td>\n",
       "      <td>339</td>\n",
       "      <td>215</td>\n",
       "      <td>1127</td>\n",
       "      <td>208</td>\n",
       "      <td>81</td>\n",
       "      <td>3244</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cv398_17047</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1896</td>\n",
       "      <td>1067</td>\n",
       "      <td>19</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>805</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cv588_13008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>18</td>\n",
       "      <td>685</td>\n",
       "      <td>250</td>\n",
       "      <td>2</td>\n",
       "      <td>7721</td>\n",
       "      <td>187</td>\n",
       "      <td>81</td>\n",
       "      <td>1911</td>\n",
       "      <td>1812</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cv734_21568</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1242</td>\n",
       "      <td>46800</td>\n",
       "      <td>21</td>\n",
       "      <td>386</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>7810</td>\n",
       "      <td>4397</td>\n",
       "      <td>1950</td>\n",
       "      <td>5476</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cv491_12145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1736</td>\n",
       "      <td>9559</td>\n",
       "      <td>4</td>\n",
       "      <td>267</td>\n",
       "      <td>1226</td>\n",
       "      <td>1895</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>1708</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>cv647_13691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2556</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>4</td>\n",
       "      <td>4143</td>\n",
       "      <td>676</td>\n",
       "      <td>2</td>\n",
       "      <td>1375</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cv665_29538</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 739 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1     2     3     4     5     6     7      8      9  ...  \\\n",
       "0       106    106   106     8    26   637   136     5    266     70  ...   \n",
       "1       163      9     1  1711  1550     4     2   101   1008   8629  ...   \n",
       "2      8124      6    25     2    27     9     6     2  17379  17380  ...   \n",
       "3     20940      1  1446   337   119  1760     1  4788     76    232  ...   \n",
       "4        43    463  5133   339   215  1127   208    81   3244      7  ...   \n",
       "...     ...    ...   ...   ...   ...   ...   ...   ...    ...    ...  ...   \n",
       "1995     26      4     1  1896  1067    19   101     3      1    805  ...   \n",
       "1996     18    685   250     2  7721   187    81  1911   1812      5  ...   \n",
       "1997   1242  46800    21   386    30     2  7810  4397   1950   5476  ...   \n",
       "1998     51      1  1736  9559     4   267  1226  1895     60     20  ...   \n",
       "1999   2556      1   131     4  4143   676     2  1375      3     78  ...   \n",
       "\n",
       "      729  730  731   732   733  734  735  736           id  sentiment  \n",
       "0       4  109  977  1633     5  129  264   11  cv676_22202         -1  \n",
       "1       0    0    0     0     0    0    0    0  cv839_22807         -1  \n",
       "2       0    0    0     0     0    0    0    0   cv155_7845         -1  \n",
       "3       0    0    0     0     0    0    0    0  cv465_23401         -1  \n",
       "4       0    0    0     0     0    0    0    0  cv398_17047         -1  \n",
       "...   ...  ...  ...   ...   ...  ...  ...  ...          ...        ...  \n",
       "1995    0    0    0     0     0    0    0    0  cv588_13008          1  \n",
       "1996    0    0    0     0     0    0    0    0  cv734_21568          1  \n",
       "1997    0    0    0     0     0    0    0    0  cv491_12145          1  \n",
       "1998   40    1  148     3  1708    7    9    0  cv647_13691          1  \n",
       "1999    0    0    0     0     0    0    0    0  cv665_29538          1  \n",
       "\n",
       "[2000 rows x 739 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I tried post-padding and post-truncate here\n",
    "sequences = t.texts_to_sequences(docs)\n",
    "\n",
    "pad_L_trun = pad_sequences(sequences, padding = 'post', maxlen = L, truncating = 'post')\n",
    "df_pad_L_trun = pd.DataFrame(pad_L_trun)\n",
    "\n",
    "df_pad_L_trun['id'] = new_review['id'].copy()\n",
    "df_pad_L_trun['sentiment'] = new_review['Sentiment'].copy()\n",
    "df_pad_L_trun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. One can use tokenized text as inputs to a deep neural network. However, a recent breakthrough in NLP suggests that more sophisticated representations of text yield better results. These sophisticated representations are called word embeddings. \n",
    "“Word embedding is a term used for representation of words for text analysis, typically in the form of a realvalued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.”.\n",
    "### Most deep learning modules (including Keras) provide a convenient way to convert positive integer representations of words into a word embedding by an “Embedding layer.” The layer accepts arguments that define the mapping of words into embeddings, including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value). The layer also allows you to specify the dimension for each word vector, called the “output dimension.” We would like to use a word embedding layer for this project. Assume that we are interested in the top 5,000 words. This means that in each integer sequence that represents each document, we set to zero those integers that represent words that are not among the top 5,000 words in the document. If you feel more adventurous, use all the words that appear in this corpus. Choose the length of the embedding vector for each word to be 32. Hence, each document is represented as a 32 × 500 matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "max_words = 500\n",
    "\n",
    "# get the reviews of train and test\n",
    "docs_train = Train['review'].tolist()\n",
    "docs_test = Test['review'].tolist()\n",
    "\n",
    "# one-hot encoding\n",
    "tokenizer = Tokenizer(num_words = top_words)\n",
    "tokenizer.fit_on_texts(docs_train)\n",
    "X_train_1 = tokenizer.texts_to_sequences(docs_train)\n",
    "X_test_1 = tokenizer.texts_to_sequences(docs_test)\n",
    "\n",
    "# padding\n",
    "X_train = pad_sequences(X_train_1, padding = 'post', maxlen = max_words)\n",
    "X_test = pad_sequences(X_test_1, padding = 'post', maxlen = max_words)\n",
    "\n",
    "# sentiment\n",
    "y_train = Train['Sentiment'].replace((1, -1), (1, 0))\n",
    "y_test = Test['Sentiment'].replace((1, -1), (1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(X_train, X_test, y_train, y_test, top_words, max_words):\n",
    "    \n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words , 32, input_length = max_words))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data = (X_test, y_test))\n",
    "    \n",
    "    # embedding train and test\n",
    "    embedding_train = model.predict(X_train)\n",
    "    embedding_test = model.predict(X_test)\n",
    "    print(embedding_train.shape, embedding_test.shape)\n",
    "    \n",
    "    return  model, embedding_train, embedding_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500, 1)            33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "44/44 [==============================] - 1s 7ms/step - loss: 0.6923 - accuracy: 0.5163 - val_loss: 0.6907 - val_accuracy: 0.5353\n",
      "(1400, 500, 1) (600, 500, 1)\n"
     ]
    }
   ],
   "source": [
    "embedded_layer_model1, embedding_train1, embedding_test1 = embedding_layer(X_train, X_test, y_train, y_test, \n",
    "                                                                           top_words, max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Flatten the matrix of each document to a vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer_flatten(X_train, X_test, y_train, y_test, top_words, max_words):\n",
    "    \n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words , 32, input_length = max_words))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data = (X_test, y_test))\n",
    "    \n",
    "    # embedding train and test\n",
    "    embedding_train = model.predict(X_train)\n",
    "    embedding_test = model.predict(X_test)\n",
    "    print(embedding_train.shape, embedding_test.shape)\n",
    "    \n",
    "    return  model, embedding_train, embedding_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16000)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 16001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 176,001\n",
      "Trainable params: 176,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "44/44 [==============================] - 1s 5ms/step - loss: 0.6917 - accuracy: 0.5229 - val_loss: 0.6875 - val_accuracy: 0.5400\n",
      "(1400, 1) (600, 1)\n"
     ]
    }
   ],
   "source": [
    "embedded_flatten_model, embedding_train_flatten, embedding_test_flatten= embedding_layer_flatten(X_train, \n",
    "                                                                                                 X_test, \n",
    "                                                                                                 y_train, \n",
    "                                                                                                 y_test,\n",
    "                                                                                                 top_words, \n",
    "                                                                                                 max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs and one output layer with a single sigmoid neuron. Use a dropout rate of 20% for the first layer and 50% for the other layers. Use ADAM optimizer and binary cross entropy loss (which is equivalent to having a softmax in the output). To avoid overfitting, just set the number of epochs as 2. Use a batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    \n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words , 32, input_length = max_words))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # the first layer\n",
    "    # 50 ReLUs\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    # Dropout rate 20% to \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # the second layer\n",
    "    # 50 ReLUs\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    # Dropout rate 50%\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # the third layer\n",
    "    # 50 ReLUs\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    # Dropout rate 50%\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # model optimizer\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 2, batch_size = 10)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    train_score = model.evaluate(X_train, y_train)[1]\n",
    "    test_score = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "    return model, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.6955 - accuracy: 0.5050 - val_loss: 0.6884 - val_accuracy: 0.5400\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.6170 - accuracy: 0.6621 - val_loss: 0.6884 - val_accuracy: 0.6250\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16000)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                800050    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 965,201\n",
      "Trainable params: 965,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.9057\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "mlp_model, train_score_mlp, test_score_mlp = mlp_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training set is: 0.9057142734527588\n",
      "The accuracy of testing set is: 0.625\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of training set is: ' + str(train_score_mlp))\n",
    "print('The accuracy of testing set is: ' + str(test_score_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) One-Dimensional Convolutional Neural Network:\n",
    "Although CNNs are mainly used for image data, they can also be applied to text data, as text also has adjacency information. Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively.\n",
    "### i. After the embedding layer, insert a Conv1D layer. This convolutional layer has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded word representations 3 vector elements of the word embedding at a time. The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_1d_model(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    \n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words , 32, input_length = max_words))\n",
    "    #model.add(Flatten())\n",
    "\n",
    "    # Conv1D layer\n",
    "    model.add(Conv1D(32, 3))\n",
    "    \n",
    "    # 1D max pooling layer\n",
    "    model.add(MaxPooling1D(pool_size = 2, strides = 2))\n",
    "    \n",
    "    # 3rd\n",
    "    # 50 ReLUs\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    # Dropout rate 20% to \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # 4th\n",
    "    # 50 ReLUs\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    # Dropout rate 50%\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # 5th\n",
    "    # 50 ReLUs\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    # Dropout rate 50%\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # model optimizer\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 2, batch_size = 10)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    train_score = model.evaluate(X_train, y_train)[1]\n",
    "    test_score = model.evaluate(X_test, y_test)[1]\n",
    "    \n",
    "    return model, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 2s 10ms/step - loss: 0.6933 - accuracy: 0.5020 - val_loss: 0.6915 - val_accuracy: 0.5432\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.6852 - accuracy: 0.5601 - val_loss: 0.6801 - val_accuracy: 0.5627\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 498, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 249, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 249, 50)           1650      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 249, 50)           0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 249, 50)           2550      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 249, 50)           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 249, 50)           2550      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 249, 50)           0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 249, 1)            51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169,905\n",
      "Trainable params: 169,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "44/44 [==============================] - 0s 4ms/step - loss: 0.6688 - accuracy: 0.5983\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6801 - accuracy: 0.5627\n"
     ]
    }
   ],
   "source": [
    "cnn_model, train_score_cnn, test_score_cnn = cnn_1d_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training set is: 0.5983334183692932\n",
      "The accuracy of testing set is: 0.5626572966575623\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of training set is: ' + str(train_score_cnn))\n",
    "print('The accuracy of testing set is: ' + str(test_score_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Long Short-Term Memory Recurrent Neural Network:\n",
    "The structure of the LSTM we are going to use is shown in the following figure.\n",
    "### i. Each word is represented to LSTM as a vector of 32 elements and the LSTM is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(X_train, y_train, X_test, y_test, num_epochs):\n",
    "    \n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words , 32, input_length = max_words))\n",
    "    #model.add(Flatten())\n",
    "    \n",
    "    # LSTM\n",
    "    # 32 elements\n",
    "    model.add(LSTM(32))\n",
    "    # Dropout rate 20% to \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # dense layer\n",
    "    # 256 ReLUs\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    # Dropout rate 20% to \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # model optimizer\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs = num_epochs, batch_size = 10, verbose = 0)\n",
    "    \n",
    "    train_score = model.evaluate(X_train, y_train)[1]\n",
    "    test_score = model.evaluate(X_test, y_test)[1]\n",
    "    \n",
    "    return model, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2834 - accuracy: 0.8236\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 1.3945 - accuracy: 0.5933\n",
      "44/44 [==============================] - 2s 29ms/step - loss: 0.2741 - accuracy: 0.8279\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 1.1861 - accuracy: 0.6033\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2800 - accuracy: 0.8257\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 1.4729 - accuracy: 0.6167\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2902 - accuracy: 0.8157\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 1.0255 - accuracy: 0.6500\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2772 - accuracy: 0.8279\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.2668 - accuracy: 0.6167\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2899 - accuracy: 0.8207\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 0.9357 - accuracy: 0.6317\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.2788 - accuracy: 0.8271\n",
      "19/19 [==============================] - 0s 23ms/step - loss: 1.1541 - accuracy: 0.6083\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.2902 - accuracy: 0.8193\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 1.1014 - accuracy: 0.6300\n",
      "44/44 [==============================] - 61s 1s/step - loss: 0.2441 - accuracy: 0.9007\n",
      "19/19 [==============================] - 0s 22ms/step - loss: 1.2906 - accuracy: 0.6983\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2028 - accuracy: 0.9243\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 1.2059 - accuracy: 0.6967\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2863 - accuracy: 0.8107\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 1.2355 - accuracy: 0.6067\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2426 - accuracy: 0.8943\n",
      "19/19 [==============================] - 0s 25ms/step - loss: 1.2648 - accuracy: 0.6600\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2900 - accuracy: 0.7836\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 1.1700 - accuracy: 0.5717\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2800 - accuracy: 0.8093\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 1.5593 - accuracy: 0.5933\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2905 - accuracy: 0.8236\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 1.3594 - accuracy: 0.5367\n",
      "44/44 [==============================] - 1s 22ms/step - loss: 0.2656 - accuracy: 0.8400\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.1628 - accuracy: 0.6800\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.0311 - accuracy: 0.9914\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 2.5737 - accuracy: 0.5217\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2669 - accuracy: 0.8257\n",
      "19/19 [==============================] - 0s 25ms/step - loss: 2.3255 - accuracy: 0.5883\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2276 - accuracy: 0.9000\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 1.2978 - accuracy: 0.6933\n",
      "44/44 [==============================] - 1s 22ms/step - loss: 0.2832 - accuracy: 0.8250\n",
      "19/19 [==============================] - 0s 22ms/step - loss: 1.6410 - accuracy: 0.6167\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2964 - accuracy: 0.8257\n",
      "19/19 [==============================] - 0s 24ms/step - loss: 0.8171 - accuracy: 0.6150\n",
      "44/44 [==============================] - 1s 22ms/step - loss: 0.1968 - accuracy: 0.9179\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 1.2979 - accuracy: 0.6983\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2804 - accuracy: 0.8236\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 1.5628 - accuracy: 0.6067\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2744 - accuracy: 0.8321\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 1.5773 - accuracy: 0.6433\n",
      "44/44 [==============================] - 1s 22ms/step - loss: 0.2849 - accuracy: 0.7750\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 1.4101 - accuracy: 0.5850\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2908 - accuracy: 0.8079\n",
      "19/19 [==============================] - 1s 32ms/step - loss: 1.8857 - accuracy: 0.5733\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.6346 - accuracy: 0.7007\n",
      "19/19 [==============================] - 1s 31ms/step - loss: 0.6762 - accuracy: 0.5667\n",
      "44/44 [==============================] - 1s 22ms/step - loss: 0.2555 - accuracy: 0.8421\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 2.3417 - accuracy: 0.5683\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2569 - accuracy: 0.8464\n",
      "19/19 [==============================] - 1s 28ms/step - loss: 1.5048 - accuracy: 0.6450\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2881 - accuracy: 0.8193\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.1852 - accuracy: 0.6467\n",
      "44/44 [==============================] - 1s 22ms/step - loss: 0.1805 - accuracy: 0.9343\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 0.9688 - accuracy: 0.7267\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2040 - accuracy: 0.9171\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 1.4400 - accuracy: 0.6900\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2095 - accuracy: 0.9086\n",
      "19/19 [==============================] - 0s 25ms/step - loss: 1.0238 - accuracy: 0.6667\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2613 - accuracy: 0.8450\n",
      "19/19 [==============================] - 0s 25ms/step - loss: 1.2180 - accuracy: 0.6650\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2638 - accuracy: 0.8271\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 1.1912 - accuracy: 0.6483\n",
      "44/44 [==============================] - 1s 23ms/step - loss: 0.2801 - accuracy: 0.8229\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.4490 - accuracy: 0.6450\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.2798 - accuracy: 0.8293\n",
      "19/19 [==============================] - 1s 27ms/step - loss: 1.1131 - accuracy: 0.6267\n",
      "44/44 [==============================] - 1s 24ms/step - loss: 0.2686 - accuracy: 0.8143\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 1.0780 - accuracy: 0.6200\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2615 - accuracy: 0.9043\n",
      "19/19 [==============================] - 0s 25ms/step - loss: 1.4613 - accuracy: 0.6883\n",
      "44/44 [==============================] - 1s 22ms/step - loss: 0.2788 - accuracy: 0.8314\n",
      "19/19 [==============================] - 1s 26ms/step - loss: 1.8783 - accuracy: 0.5617\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.1787 - accuracy: 0.9293\n",
      "19/19 [==============================] - 1s 35ms/step - loss: 1.9781 - accuracy: 0.6450\n"
     ]
    }
   ],
   "source": [
    "# try different epochs values\n",
    "name = locals()\n",
    "ls_ac_lstm = []\n",
    "for i in range(10, 51):\n",
    "    name['lstm_model' + str(i)], train_score_lstm, test_score_lstm = lstm_model(X_train, y_train, X_test, y_test, i)\n",
    "    ls_ac_lstm.append([i, train_score_lstm, test_score_lstm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>40</td>\n",
       "      <td>0.934286</td>\n",
       "      <td>0.726667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31</td>\n",
       "      <td>0.917857</td>\n",
       "      <td>0.698333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18</td>\n",
       "      <td>0.900714</td>\n",
       "      <td>0.698333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>0.924286</td>\n",
       "      <td>0.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>28</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.693333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>41</td>\n",
       "      <td>0.917143</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>48</td>\n",
       "      <td>0.904286</td>\n",
       "      <td>0.688333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>42</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>43</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>0.894286</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0.815714</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>44</td>\n",
       "      <td>0.827143</td>\n",
       "      <td>0.648333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>39</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.646667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50</td>\n",
       "      <td>0.929286</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>45</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>38</td>\n",
       "      <td>0.846429</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>33</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.643333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.820714</td>\n",
       "      <td>0.631667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>46</td>\n",
       "      <td>0.829286</td>\n",
       "      <td>0.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>47</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>0.827857</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0.825714</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30</td>\n",
       "      <td>0.825714</td>\n",
       "      <td>0.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>0.827143</td>\n",
       "      <td>0.608333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.606667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>0.810714</td>\n",
       "      <td>0.606667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.827857</td>\n",
       "      <td>0.603333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23</td>\n",
       "      <td>0.809286</td>\n",
       "      <td>0.593333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.593333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>27</td>\n",
       "      <td>0.825714</td>\n",
       "      <td>0.588333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>34</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>35</td>\n",
       "      <td>0.807857</td>\n",
       "      <td>0.573333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22</td>\n",
       "      <td>0.783571</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>37</td>\n",
       "      <td>0.842143</td>\n",
       "      <td>0.568333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>36</td>\n",
       "      <td>0.700714</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>49</td>\n",
       "      <td>0.831429</td>\n",
       "      <td>0.561667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.536667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26</td>\n",
       "      <td>0.991429</td>\n",
       "      <td>0.521667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_accuracy  test_accuracy\n",
       "30     40        0.934286       0.726667\n",
       "21     31        0.917857       0.698333\n",
       "8      18        0.900714       0.698333\n",
       "9      19        0.924286       0.696667\n",
       "18     28        0.900000       0.693333\n",
       "31     41        0.917143       0.690000\n",
       "38     48        0.904286       0.688333\n",
       "15     25        0.840000       0.680000\n",
       "32     42        0.908571       0.666667\n",
       "33     43        0.845000       0.665000\n",
       "11     21        0.894286       0.660000\n",
       "3      13        0.815714       0.650000\n",
       "34     44        0.827143       0.648333\n",
       "29     39        0.819286       0.646667\n",
       "40     50        0.929286       0.645000\n",
       "35     45        0.822857       0.645000\n",
       "28     38        0.846429       0.645000\n",
       "23     33        0.832143       0.643333\n",
       "5      15        0.820714       0.631667\n",
       "7      17        0.819286       0.630000\n",
       "36     46        0.829286       0.626667\n",
       "37     47        0.814286       0.620000\n",
       "19     29        0.825000       0.616667\n",
       "4      14        0.827857       0.616667\n",
       "2      12        0.825714       0.616667\n",
       "20     30        0.825714       0.615000\n",
       "6      16        0.827143       0.608333\n",
       "22     32        0.823571       0.606667\n",
       "10     20        0.810714       0.606667\n",
       "1      11        0.827857       0.603333\n",
       "13     23        0.809286       0.593333\n",
       "0      10        0.823571       0.593333\n",
       "17     27        0.825714       0.588333\n",
       "24     34        0.775000       0.585000\n",
       "25     35        0.807857       0.573333\n",
       "12     22        0.783571       0.571667\n",
       "27     37        0.842143       0.568333\n",
       "26     36        0.700714       0.566667\n",
       "39     49        0.831429       0.561667\n",
       "14     24        0.823571       0.536667\n",
       "16     26        0.991429       0.521667"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the test accuracy to select the best model\n",
    "df_ac_lstm = pd.DataFrame(ls_ac_lstm, columns = ['epoch', 'train_accuracy', 'test_accuracy'])\n",
    "df_ac_lstm = df_ac_lstm.sort_values('test_accuracy', ascending = False)\n",
    "df_ac_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see from the result, the best model is \"epochs = 40.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training set is: 0.9342857003211975\n",
      "The accuracy of testing set is: 0.7266666889190674\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of training set is: ' + str(df_ac_lstm['train_accuracy'][:1].values[0]))\n",
    "print('The accuracy of testing set is: ' + str(df_ac_lstm['test_accuracy'][:1].values[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
